# -*- coding: utf-8 -*-
"""MLINTERN TASK.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VYgMD_3BDcc9CTD7w408VaGFihpiwNEK

TASK1
"""

# Import libraries for data manipulation and visualization
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset from the CSV file
df = pd.read_csv("TASK-ML-INTERN.csv")

"""Inspect dataset"""

df.info()

df.head()

print("Original dataset shape:", df.shape)
df = df[df["vomitoxin_ppb"] <= 8000]
print("Filtered dataset shape:", df.shape)

"""# Identify spectral columns"""

spectral_cols = [col for col in df.columns if col not in ['hsi_id', 'vomitoxin_ppb']]
missing_vals = df[spectral_cols].isna().sum()

print(missing_vals[missing_vals > 0])# no missing val

df[spectral_cols] = df[spectral_cols].fillna(df[spectral_cols].mean())

#standardising data
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df[spectral_cols] = scaler.fit_transform(df[spectral_cols])

"""visualization spectral bands

avg reflectance line
"""

avg_reflectance = df[spectral_cols].mean()
plt.figure(figsize=(12, 6))
plt.plot(range(len(avg_reflectance)), avg_reflectance, marker='o', linestyle='-')
plt.title("Average Spectral Reflectance Across Wavelength Bands")
plt.xlabel("Wavelength Band Index")
plt.ylabel("Normalized Reflectance")
plt.grid(True)
plt.show()

"""heatmap of reflectance (subset of samples)"""

plt.figure(figsize=(14, 8))
sns.heatmap(df[spectral_cols].iloc[:50], cmap="viridis")
plt.title("Heatmap of Spectral Reflectance for 50 Samples")
plt.xlabel("Wavelength Band Index")
plt.ylabel("Sample Index")
plt.show()

"""**TASK 2**"""

#APPLYING pca
from sklearn.decomposition import PCA

# Apply PCA on the normalized spectral data (using all spectral columns)
pca = PCA(n_components=10)  # Extracting the top 10 principal components
pca_result = pca.fit_transform(df[spectral_cols])

explained_variance = pca.explained_variance_ratio_
print("\nVariance explained by the top 10 principal components:")
for i, ev in enumerate(explained_variance, 1):
    print(f"PC{i}: {ev:.4f}")

"""fist 2 principal components plot"""

plt.figure(figsize=(10, 6))
plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.7)
plt.title("2D Scatter Plot of PCA: PC1 vs PC2")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.grid(True)
plt.show()

"""**TASK3**

MODEL BUILDING
"""

from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

"""define data"""

X = df[spectral_cols].values
y = df['vomitoxin_ppb'].values



# Split into training and testing  sets(80-20)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Reshape the input data for 1D CNN
X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test_cnn  = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))
print("Shape of CNN input data (training):", X_train_cnn.shape)

"""model architecture"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense, BatchNormalization, Input
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)

model = Sequential([
    Input(shape=(X_train.shape[1], X_train.shape[2])), # Now X_train.shape[2] is valid

    # First conv block with L2 regularization and batch norm
    Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.001)),
    BatchNormalization(),
    MaxPooling1D(pool_size=2),
    Dropout(0.4),

    # Second conv block
    Conv1D(filters=128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.001)),
    BatchNormalization(),
    MaxPooling1D(pool_size=2),
    Dropout(0.4),

    Flatten(),

    # Dense layers
    Dense(64, activation='relu'),
    Dropout(0.4),
    Dense(1)  # Final regression output
])

from tensorflow.keras.optimizers import Adam
optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])

early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the CNN model
history = model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=150,
    batch_size=16,
    verbose=1
)

"""**TASK4**

**MODEL EVALUATION**
"""

# Evaluate the CNN model on the test set
loss, mae = model.evaluate(X_test, y_test)
y_pred = model.predict(X_test).flatten()

import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Replace infinite values in y_test with NaN
y_test = np.nan_to_num(y_test)

# Assuming y_test and y_pred are your actual and predicted values
# Replace any infinite or very large values in y_pred with a finite value
y_pred = np.nan_to_num(y_pred)
y_pred = np.clip(y_pred, a_min=None, a_max=1e10)  # Clip very large values

# Calculate detailed regression metrics
mae_val = mean_absolute_error(y_test, y_pred)
rmse_val = np.sqrt(mean_squared_error(y_test, y_pred))
r2_val = r2_score(y_test, y_pred)

print("\n=== CNN Evaluation Metrics on Full Data ===")
print(f"MAE: {mae_val:.2f}")
print(f"RMSE: {rmse_val:.2f}")
print(f"RÂ² Score: {r2_val:.2f}")

plt.figure(figsize=(8, 8))
plt.scatter(y_test, y_pred, alpha=0.7, edgecolors='k')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--', lw=2)
plt.xlabel("Actual DON Concentration (ppb)")
plt.ylabel("Predicted DON Concentration (ppb)")
plt.title("CNN: Actual vs. Predicted DON Concentrations")
plt.grid(True)
plt.show()

"""lstm"""